# This code compares the tokenization speed and number of tokens generated by BPE and the dynamic tokenizer

import time  

# Use Hugging Face's BPE tokenizer vs. the dynamic simulation  
text_long = " ".join(["This is a sample sentence for tokenization." for _ in range(1000)])  

# Measure time and token count for BPE  
start_bpe = time.time()  
tokens_bpe = hf_tokenizer.encode_plus(text_long, return_tensors="pt")  
print(f"BPE Tokens: {len(tokens_bpe['input_ids'][0])} | Time: {time.time() - start_bpe}")  

# Measure time and token count for dynamic tokenizer  
start_dynamic = time.time()  
tokens_dynamic = dynamic_tokenizer.encode(text_long)  
print(f"T-Free Tokens: {len(tokens_dynamic)} | Time: {time.time() - start_dynamic}")  